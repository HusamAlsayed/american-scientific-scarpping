{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "webScrammping",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7kcua_WlccV"
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import threading\n",
        "import glob \n",
        "import shutil"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3yx5FTHxLAy"
      },
      "source": [
        "def get_topic_information(topic_id):\n",
        "  url = 'https://www.scientificamerican.com/the-sciences/'\n",
        "  response = requests.get(url)\n",
        "  response.encoding = 'utf-8'\n",
        "  value = response.text\n",
        "  soup = BeautifulSoup(value,'lxml')\n",
        "  state = soup.find('ul',class_ = 'header-topic-list landing-header__nav__list')\n",
        "  topics = state.find_all('li',class_ = 'landing-header__nav__list__item')\n",
        "  topic = topics[topic_id]\n",
        "  current_topic_href= topic.a['href']\n",
        "  current_topic_text = topic.a.text\n",
        "  response = requests.get(current_topic_href)\n",
        "  response.encoding = 'utf-8'\n",
        "  value = response.text\n",
        "  soup = BeautifulSoup(value,'lxml')\n",
        "  state = soup.find('ol',class_ = 'pagination__nums')\n",
        "  pages = state.find_all('li')\n",
        "  number_of_pages = int(pages[-1].text)\n",
        "  return current_topic_href , current_topic_text , number_of_pages"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2KFgOvxy8Ko"
      },
      "source": [
        "topics_info = []\n",
        "for i in range(8):\n",
        "  topic_href , topic_name,number_of_pages = get_topic_information(i)\n",
        "  topics_info.append({'topic_href':topic_href,'topic_name':topic_name,'number_of_pages':number_of_pages})"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmS661oqzSch",
        "outputId": "241658d7-637b-4965-fc77-7258bb1bb11b"
      },
      "source": [
        "topics_info[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'number_of_pages': 52,\n",
              " 'topic_href': 'https://www.scientificamerican.com/arts-and-culture/',\n",
              " 'topic_name': 'Arts & Culture'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQP1kaSAQIoO"
      },
      "source": [
        "def scrap_data(topic_name,topic_href,topic_id,from_,to_):\n",
        "  to_ = min(to_,topics_info[topic_id]['number_of_pages'])\n",
        "  arr = []\n",
        "  # print(topic_name)\n",
        "  # print('the from ' , from_ ,  'the to ' , to_)\n",
        "  for k in range(from_,to_):\n",
        "    current_page_link = topic_href + '?page=' + str(k + 1)\n",
        "    response = requests.get(current_page_link)\n",
        "    response.encoding = 'utf-8'\n",
        "    value = response.text\n",
        "    soup = BeautifulSoup(value,'lxml')\n",
        "    all_article_pages = soup.find_all('div',class_='listing-wide__inner')\n",
        "    for article_page in all_article_pages:\n",
        "      article_link = article_page.a['href']\n",
        "      title = article_page.a.text \n",
        "      response = requests.get(article_link)\n",
        "      response.encoding = 'utf-8'\n",
        "      value = response.text\n",
        "      soup = BeautifulSoup(value,'lxml')\n",
        "      divParagraph = soup.find_all('div',class_='mura-region-local')\n",
        "      video_state = soup.find('div',class_='podcasts-media podcasts-media--feature podcasts__media')\n",
        "      if video_state is not None:\n",
        "        # print('NONE')\n",
        "        continue\n",
        "      article_text = ''\n",
        "      for div in divParagraph:\n",
        "        paragraph = div.find_all('p')\n",
        "        for p in paragraph:\n",
        "          article_text+=p.text\n",
        "      arr.append([article_text,title])\n",
        "      # print(len(arr))\n",
        "  df = pd.DataFrame(arr,columns = ['articleText','title'])\n",
        "  file_idx = random.randint(1,10000)\n",
        "  folder_name = '/content/drive/MyDrive/dataset/science dataset/{}'.format(topic_name)\n",
        "  if os.path.exists(folder_name) == False:\n",
        "    os.mkdir(folder_name)\n",
        "  path = folder_name + '/{}.csv'.format(str(file_idx))\n",
        "  df.to_csv(path,index = False)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmA1lRcva8n0"
      },
      "source": [
        "def generate_threads(number_of_threads,number_of_iteration,start_topic,start_page,increment):\n",
        "  ok = 0\n",
        "  topic_id = start_page\n",
        "  current_iteration = 0\n",
        "  for it in range(number_of_iteration):\n",
        "    thr = []\n",
        "    for i in range(number_of_threads):\n",
        "      number_of_pages = int (topics_info[topic_id]['number_of_pages'])\n",
        "      topic_name = topics_info[topic_id]['topic_name']\n",
        "      topic_href = topics_info[topic_id]['topic_href']\n",
        "      st = start_page + current_iteration*number_of_threads*increment + i*increment\n",
        "      thread = threading.Thread(target = scrap_data,args = (topic_name,topic_href,topic_id,st,st + increment))\n",
        "      if st + increment >= number_of_pages:\n",
        "        ok = 1\n",
        "      thr.append(thread)\n",
        "      thr[i].start()\n",
        "    \n",
        "    current_iteration+=1\n",
        "    for i in range(number_of_threads):\n",
        "      thr[i].join()\n",
        "    if ok:\n",
        "      topic_id+=ok\n",
        "      current_iteration = 0\n",
        "      ok = 0 \n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEO6snCFz2hy",
        "outputId": "d6ee1059-6587-4e0c-b2ea-1c1b67dbd4c2"
      },
      "source": [
        "generate_threads(5,500,0,0,2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11\n",
            "16\n",
            "NONE\n",
            "11\n",
            "8\n",
            "17\n",
            "12\n",
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjMs6wa0L_yb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j78msW5flvAH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}